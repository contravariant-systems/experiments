* Contravariant Systems: Build Log

** 2025-12-20: JAX Fundamentals
*** What I Built/Learnt
**** Installing JAX and development tools

There was a time when Apple had interest in maintaining a Metal
backend for JAX, which would've allowed things to be GPU accelerated
as I learnt. But that time has long passed. So for now, it's just a
[[https://docs.jax.dev/en/latest/installation.html][standard CPU install]] following the most basic steps.

In addition, I install an LSP-server and friends because it helps with
my introspection as I learn.

#+begin_src shell
python -m venv venv
pip install -U jax
pip freeze > requirements.txt
pip install -U "python-lsp-server[all]"
pip freeze > requirements-dev-tmp.txt
diff requirements-dev-tmp.txt requirements.txt | grep  '^<' | sed -e 's/< //g' > requirements-dev.txt
rm requirements-dev-tmp.txt
pip install -U matplotlib
# Then some work to update requirements.txt in the same way as above
#+end_src

Initially, I wanted to work in Emacs but getting a proper REPL with
graphics (e.g. Matplotlib) and good understanding of =venv=s was too
much.

So I moved on to Jupyter notebooks.

It was super-duper complicated to get Jupyter notebooks working
globally while seeing python packages in the VM, so I ended up just
installing jupyter lab locally. And even then, this needed to be run
in hard-coded way:

#+begin_src shell
./venv/bin/jupyter lab
#+end_src

**** Going through JAX's quickstart and related reading

I went through the following material in a file called
=quickstart.ipynb=, which introduced me to many concepts.

- https://docs.jax.dev/en/latest/notebooks/thinking_in_jax.html

In the course of this, it spidered into many, many tabs, so I followed
through most of them at least at basic level.
- https://docs.jax.dev/en/latest/key-concepts.html#jax-arrays-jax-array
- https://docs.jax.dev/en/latest/jit-compilation.html#jit-compilation
- https://docs.jax.dev/en/latest/jaxpr.html#jax-internals-jaxpr
- https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html
- https://docs.jax.dev/en/latest/automatic-vectorization.html
- https://docs.jax.dev/en/latest/automatic-differentiation.html
- https://docs.jax.dev/en/latest/random-numbers.html

**** Creating the most basic mechanical system to exercise JAX semantics

I then tried to recreate the most basic Hamiltonian system, the simple
harmonic oscillator using JAX and studied it a little. This works
exists in =sho.py=.

*** What Surprised Me

- How useful learning Scheme and functional-ish programming has been
  to this exercise.
- How readable (because of the above) the internal =jaxpr=
  representation is.
- How rapidly forward Euler builds up energy. Turns out this is a
  unversal thing for Hamiltonian mechanics. By the same token,
  backward Euler always loses energy.
- =jax.grad= is really cool, and it works with gradients essentially
  related to all input arguments of a function. It, by default, does
  it relative to the first one, which was confusing, but you can set
  =argnums= to a list of numbers that correspond to the positions of
  the arguments.
- It works even through what feel like weird Python containers like
  dictionaries??? (This is where my lack of knowledge of =pytrees= is
  showing.)
- More deeply though, it doesn't care whether the input is a physical
  constant or variable or something else entirely. It's just an input
  and you can differentiate relative to it. It's our human semantics
  that let us distinguish between "physical constant" and "variable to
  be learnt."

*** What Is Confusing or Didn't Work

- Emacs is not a great REPL for python.
- Jupyter and venvs are more convoluted than they should be.
- Modern JAX is not metal-accelerated on Apple Silicon macs.
- The internal representation of grad(grad(grad ... f)) gets longer
  and longer, even though at some point for a polynomial f it has gone
  to 0.
- I had to use =defaults write org.python.python ApplePersistenceIgnoreState NO=
  to prevent a very silly warning.
- JAX's JIT is quite strict about changes to shapes of things at
  runtime within the body of functions. The reason for this is that it
  "traces" the code by running it, then stores the AST to act on it
  later. If it suspects something can arbitrarily change at runtime,
  it will complain. We can either mark it is as "don't worry about it,
  this is unchanging" or reconfigure the code for shapes to be
  explicit inputs.
- I still need to better understand =jax.lax.scan=.

*** Open Questions

- How do I represent structural state and hold onto it as we go from
  symbolic to JAX (MLIR) and back.
- I really do not understand pytrees

*** Refinements to the Vision

- I am starting to see that some annotations, either at the Sympy
  level or at the JAX representation level might be what I am after.
- One example of this is =state_0= is an initial condition, and we
  need to vary this for sensitivity analysis and =m= and =k= could be
  either fixed constants or things to be learnt. Either way, the
  gradient is always available.

** 2025-12-21: Differentiating Through Dynamics
*** What I Built/Learnt
**** General insights
- In general, what is going on when constructing gradients is that the
  system is using the runtime tracing mechanism to create some sort of
  AST (=jaxpr=), and then use AD in this space to construct gradients.
- More specifically, for something like grad(final_energy) works
  relative to the initial conditions we have many, many inputs and one
  (pair of) outputs. This is best handled by reverse mode-AD. This
  means that the machinery needs to store all time-steps worth of
  forward calculations, and then use those in the backward pass as it
  accumulates gradients. This can be memory-prohibitive when there are
  too many steps involved.
- This is what we get when we discretise first then attempt to
  optimise. However, we can also attempt to do "something intelligent"
  first, then discretise. This is the path followed by the Neural ODE
  paper (Chen et al., 2018) which ends up in Diffrax.
**** Replacing the Forward Eulter integrator with Runge-Kutta 4
- It seems extremely energy conserving. This is because it is O(h^4)
  accumulation error.
- https://en.wikipedia.org/wiki/Runge–Kutta_methods

**** Proceeding to the inverse problem
Instead of simply fixing the parameters, we now assume they're unknown
and will estimate them. This begins with calculating a trajectory with
some known parameters. Then we add some noise to it and use a loss
function that has the following signature:

#+begin_src python
loss(traj_observed, state_0, n_steps, dt, params_guess)
#+end_src

i.e., it takes the made up trajectory and enough to calculate a new
trajectory with a guessed parameter. Then we can differentiate this
relative to the parameter and gradient descent our way through to the
right parameter.

***** Installing Optax

The above was done by hand, and we now install an optimising library
built on JAX: https://github.com/google-deepmind/optax

#+begin_src bash
pip install optax
diff requirements-tmp.txt requirements-dev.txt | grep '^<' | sed -e 's/< //g' > requirements.txt
#+end_src

**** Experiments with the inverse problem
- When replacing RK4 with Forward Euler, it didn't seem to make a
  massive difference.
- As the noise scale increases, the optimisers are extremely robust.
- Allowing the system to optimise for both m and k simultaneously
  cauesd it to find whatever random values (k was close to the true
  value) that sort of fit.
*** What Surprised Me
- Looking at the =jaxpr=s for things is a way to understand what is
  going on. e.g. Specifically in some sort of differentiation over a
  loop sense, a single scan (fold) corresponds to two in the internal
  representation. Notice that one is forward =reverse=False= and one
  is reverse.

#+begin_src
    _:f32[2] _:f32[] g:f32[5,2] = scan[
      _split_transpose=False
      jaxpr={ lambda ; h:f32[] i:f32[] j:f32[2] k:f32[]. let
          l:f32[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] j
          m:f32[] = squeeze[dimensions=(0,)] l
          n:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] j
          o:f32[] = squeeze[dimensions=(0,)] n
          p:f32[] = div o h
          q:f32[] = mul i m
          r:f32[1] = broadcast_in_dim[
            broadcast_dimensions=()
            shape=(1,)
            sharding=None
          ] p
          s:f32[1] = broadcast_in_dim[
            broadcast_dimensions=()
            shape=(1,)
            sharding=None
          ] q
          t:f32[2] = concatenate[dimension=0] r s
          u:f32[2] = mul 0.009999999776482582:f32[] t
          v:f32[2] = add j u
          w:f32[] = add k 0.01:f32[]
        in (v, w, v) }
      length=5
      linear=(False, False, False, False)
      num_carry=2
      num_consts=2
      reverse=False
      unroll=1
    ] d f a 0.0:f32[]

    cg:f32[2] = scan[
      _split_transpose=False
      jaxpr={ lambda ; ch:f32[] ci:f32[] cj:f32[2] ck:f32[2]. let
          cl:f32[2] = add_any cj ck
          cm:f32[2] = mul 0.009999999776482582:f32[] cl
          cn:f32[1] co:f32[1] = split[axis=0 sizes=(1, 1)] cm
          cp:f32[] = reduce_sum[axes=(np.int64(0),) out_sharding=None] co
          cq:f32[] = reduce_sum[axes=(np.int64(0),) out_sharding=None] cn
          cr:f32[] = mul ci cp
          cs:f32[] = div cq ch
          ct:f32[1] = broadcast_in_dim[
            broadcast_dimensions=()
            shape=(1,)
            sharding=None
          ] cs
          cu:f32[2] = pad[padding_config=((1, np.int64(0), 0),)] ct 0.0:f32[]
          cv:f32[2] = add_any cl cu
          cw:f32[1] = broadcast_in_dim[
            broadcast_dimensions=()
            shape=(1,)
            sharding=None
          ] cr
          cx:f32[2] = pad[padding_config=((0, np.int64(1), 0),)] cw 0.0:f32[]
          cy:f32[2] = add_any cv cx
        in (cy,) }
      length=5
      linear=(False, False, True, True)
      num_carry=1
      num_consts=2
      reverse=True
      unroll=1
    ] d f cf ce
  #+end_src
- Runge-Kutta (RK4) is astonishingly good at conserving energy. After
  100000 steps, it's off by O(1.e-6).
- I can crank up noise levels on a trajectory to really high (like
  =noise_scale=500=) and not have the optimisers fail to find =k=.
- What was really, really cool is that the dynamics of the system only
  depend on =sqrt(k/m)=. So when optimising for =k= and =m=
  simultaneously, even though =m= didn't stay fixed, the ratio of
  these two were bang on at the reference values.
*** What Is Confusing or Didn't Work
**** Loss functions seem kinda arbitrary
They are scalars that depend on some sort of unsigned distance to some
truth, but it feels quite unphysical to me to arbitrarily add
quantities of different types into one scalar.
**** =jax.linalg.norm= with =ord=2= gets the wrong gradient sign
I need to better understand how the norm is defined, and I need to
understand why it was messing with the sign of the gradient of the
loss.
**** The Adam optimiser is quite finicky
- To freeze parameters, we need to mark them with =True=.
- With a low learning rate, convergence was super slow. Then with a
  higher rate, it converged rapidly but oscillated. Standard gradient
  descent was so much smoother.
*** Open Questions
*** Refinements to the Vision
- We need to enforce what is known, otherwise we will find equivalent
  solutions if a problem is undefined.
** 2025-12-22: Sympy to JAX Bridge

This is the core of Contravariant's architecture: symbolic
specification → compiled numerics.

*** What I Built/Learnt
**** Went through the introductory documentation on Sympy
- https://docs.sympy.org/latest/tutorials/intro-tutorial/index.html
- https://docs.sympy.org/latest/tutorials/intro-tutorial/gotchas.html
- https://docs.sympy.org/latest/explanation/gotchas.html#gotchas
- https://docs.sympy.org/latest/tutorials/intro-tutorial/basic_operations.html
- The outcome of these experiments went into =sympy-quickstart.ipynb=.
***** Installing Sympy
#+begin_src bash
pip install sympy
pip freeze > requirements-tmp.txt
diff requirements-tmp.txt requirements-dev.txt | grep '^<' | sed -e 's/< //g' > requirements.txt
rm requirements-tmp.txt
#+end_src

Remember that we need to start it with the local =sympy=.

#+begin_src bash
./venv/bin/jupyter lab
#+end_src
***** Converting from Sympy to Numpy

This is the crux of the conversion. In =sympy=, you can go from plain
text to sympy expressions with =sympify()=. And when you need to
evaluate things, you can use =subs()= followed by =evalf()= but this
is not fast.

What we actually want to do is to convert from a =sympy=
representation to a more standard Python (=numpy=) function
representation, which can then be easily applied to a large set of
input numbers.

This essentially just converts names of things to be relevant to where
you're going to use the function. In general, SymPy functions do not
work with objects from other libraries, such as NumPy arrays, and
functions from numeric libraries like NumPy or mpmath do not work on
SymPy expressions. =lambdify= bridges the two by converting a SymPy
expression to an equivalent numeric function.

#+begin_src python
a = numpy.arange(20)
expr = sin(x)
f = lambdify(x, expr, "numpy")
f(a)
#+end_src

**** Describing the physics in terms of Lagrangians in Sympy
This was fairly straightforward, and compared very favourably with
doing it by hand.
*** What Surprised Me
- Turns out it is undecidable as to whether two symbolic expressions
  are always equal to each other:
  https://en.wikipedia.org/wiki/Richardson%27s_theorem
- This is the deepest thing I learnt today. When we define a
  Lagrangian, it is a function on state space, i.e. it is
  parameterised by the the space of =(q, q_qdot)= pairs. You can ask
  it questions like, given a position and a velocity, what is the
  Lagrangian?

  Notice you are making no claims as to whether this is a valid
  trajectory in terms of satisfying any physics. It's just a function
  in 2D space. In this context, =q= and =q_dot= are independent
  variables, and independent of one another.

  Once you've figured out a specific trajectory =q(t)=, then
  =q_dot(t)= is fully defined as just the time derivative of =q=. In
  this context, they are no longer independent.

  Now the genius bit here is that it's the Euler-Lagrange equation
  that lives in the boundary between these two contexts. The partial
  derivatives in the equation =dL/dq= and =dL/dq_dot= are evaluated
  in the first (independent) context but the total time derivative is
  evaluated in the second context (along a trajectory)!

  The Euler-Lagrange equation is the stationarity condition, i.e.
  given we're constrained to paths where the velocity is the time
  derivative of the position, which paths make the action stationary.

  The variational principle selects, from all possible paths, those
  that are physically realisable, where the velocity really is the
  derivative of position, and the acceleration really is what the
  forces dictate. Oh my god.
- The move from using Hamiltonians =(q, p)= to Lagrangians =(q, q_dot)=
  was almost trivial in the numerical integrator code. In both
  cases, we are trying to evolve the state in state space with the
  dynamics being defined in terms of the time derivative of this. I
  expected more work in breaking up the second-order polynomial into
  two first order ones.

*** What Is Confusing or Didn't Work
It was really, really hard to work with trajectory-matching loss
functions. This means that it's really hard to solve optimisation
problems quickly and generally in this space. There appear to be a lot
of local minima.

*** Open Questions
- I still need to understand where in the sympy -> jax representation
  (a sidecar dictionary?) that I store physics insights about where
  terms come from.
*** Refinements to the Vision
The core of the idea looks something like:

#+begin_src python
# Symbolic specification
L = Rational(1,2)*m*(q1_dot**2 + q2_dot**2) - Rational(1,2)*k*(q1**2 + q2**2)

# Structural analysis (Euler-Lagrange)
eom = derive_equations_of_motion(L, [q1, q2], [q1_dot, q2_dot])

# Code generation
dynamics = make_dynamics_from_eom(eom)

# Compiled, differentiable simulation
traj = integrate_rk4(state_0, n_steps, dt, params, dynamics)

# Learning
grad_loss = grad(loss, argnums=4)(traj_observed, ..., dynamics)
#+end_src
** 2025-12-23: Structure-Preserving Integration

RK4 is accurate but not structure-preserving. Over long times, it
drifts. Today we built an integrator that respects the geometry of
Hamiltonian mechanics.

Compare gradient quality: does structure preservation help learning?

*** What I Built/Learnt
**** Reading up about integrators for Hamiltonian systems
- https://en.wikipedia.org/wiki/Symplectic_integrator

  Symplectic integrators possess, as a conserved quantity, a
  Hamiltonian which is slightly perturbed from the original one. By
  virtue of these advantages, the SI scheme has been widely applied to
  the calculations of long-term evolution of chaotic Hamiltonian
  systems ranging from the Kepler problem to the classical and
  semi-classical simulations in molecular dynamics.

  Most of the usual numerical methods, such as the primitive Euler
  scheme and the classical Runge–Kutta scheme, are not symplectic
  integrators.

- https://en.wikipedia.org/wiki/Symplectic_integrator
- https://en.wikipedia.org/wiki/Verlet_integration

  The Verlet integrator provides good numerical stability, as well as
  other properties that are important in physical systems such as time
  reversibility and preservation of the symplectic form on phase
  space, at no significant additional computational cost over the
  simple Euler method.

  The form we end up using is called the Velocity Verlet.

- https://link.springer.com/book/10.1007/3-540-30666-8
- https://www.unige.ch/~hairer/poly_geoint/week1.pdf
- https://mitp-content-server.mit.edu/books/content/sectbyfn/books_pres_0/9579/sicm_edition_2.zip/chapter003.html#h1-18

  Our motivation for the development of Hamilton’s equations was to
  focus attention on the quantities that can be conserved—the momenta
  and the energy. In the Hamiltonian formulation the generalized
  configuration coordinates and the conjugate momenta comprise the
  state of the system at a given time. We know from the Lagrangian
  formulation that if the Lagrangian does not depend on some
  coordinate then the conjugate momentum is conserved. This is also
  true in the Hamiltonian formulation, but there is a distinct
  advantage to the Hamiltonian formulation. In the Lagrangian
  formulation the knowledge of the conserved momentum does not lead
  immediately to any simplification of the problem, but in the
  Hamiltonian formulation the fact that momenta are conserved gives an
  immediate reduction in the dimension of the system to be solved. In
  fact, if a coordinate does not appear in the Hamiltonian then the
  dimension of the system of coupled equations that remain to be
  solved is reduced by two: the coordinate does not appear and the
  conjugate momentum is constant.

- https://mitp-content-server.mit.edu/books/content/sectbyfn/books_pres_0/9579/sicm_edition_2.zip/chapter004.html
- https://scicomp.stackexchange.com/questions/29149/what-does-symplectic-mean-in-reference-to-numerical-integrators-and-does-scip
**** Some other things I learnt in my reading
***** What is the symplectic 2-form and why does preserving it matter?
The symplectic 2-form \omega is the intrinsic geometric structure of
phase space that turns dH into dynamics and defines the
Poisson/bracket/area geometry; preserving it means the evolution is a
true Hamiltonian transformation—volume- and structure-preserving—so it
keeps the system’s qualitative mechanics faithful, especially over
long times.

***** What does "bounded energy error" mean precisely?
As we run a symplectic integrator over time, the deviation
H(z_n)-H(z_0) stays uniformly bounded in n (typically by something
small in h), rather than accumulating unboundedly with time. The
bounded error isn't just bounded—it's /oscillatory/. The integrator
exactly preserves a slightly perturbed Hamiltonian $\tilde{H} = H +
O(h^k)$, so the true energy oscillates around the initial value rather
than drifting in one direction.

***** Why does Verlet work for separable Hamiltonians specifically?
Verlet’s magic is that separability gives you two exact, cheap
Hamiltonian flows (“kick” and “drift”), and Verlet is the symmetric
composition that keeps the geometry.

***** What goes wrong for non-separable Hamiltonians?

Verlet’s whole construction relies on the existence of two exact,
explicit, symplectic subflows. Non-separability destroys that, and
naive adaptations typically stop being symplectic and start drifting
qualitatively over long times. You will need other constructions like
symplectic versions of Runge-Kutta.

**** Completely reorganised the code
Separated the code out into a core module that can be included and
reused.

#+begin_src bash
contravariant
├── __init__.py
├── codegen.py
├── integrators.py
├── learning.py
├── plotting.py
└── symbolic.py
#+end_src

But in doing this, the language model generated all the code for the
different sections, so need to proofread and polish it.

**** Studying Verlet vs RK4 in different circumstances
With the newly modularised code, it was very easy to run the:

- Simple harmonic oscillator
- 2D Isotropic oscillator
- Coupled oscillator

For small dt and moderate times, RK4's higher accuracy dominates. The
symplectic advantage shows when:

- Long times: drift accumulates
- Larger timesteps: accuracy per step matters less, structure matters more
- Chaotic systems: where small errors compound exponentially

**** Studying the non-separable case

Verlet requires H = T(p) + V(q) (separable). We then try the double
pendulum where this condition is not met.

First, the system is smart enough to recognise this(!) when it figures
out that the terms in the Lagrangian are not separable. This is good
for Contravariant as this is the sort of thing we will use to identify
methods.

But when we run it via Verlet anyway on the non-separable double
pendulum problem, it still tries to oscillate about something, except
it's the wrong solution.

In summary:

- Coupling in V(q): fine for Verlet. The potential can mix positions
  however it wants.
- Coupling in T(q,p): breaks Verlet. The kinetic energy must be
  T(p) only.

**** Where structure preservation does help learning?
- Learning from conserved quantities: If you match energy instead of
  trajectory, Verlet's bounded energy error means the loss surface is
  smoother.
- Long-horizon prediction: If you're learning a model to predict far
  into the future, Verlet won't accumulate drift that corrupts your
  training signal.
- Chaotic systems: RK4 on a chaotic system gives nonsense gradients
  after the Lyapunov time. Verlet at least preserves the statistical
  structure.

The deeper point: structure preservation helps when your loss function
respects the structure. Trajectory matching doesn't. Energy matching
does

*** What Surprised Me
- Verlet exactly preserves a nearby Hamiltonian H~=H+O(h2)\tilde{H} =
  H + O(h^2) H~=H+O(h2), so the true energy oscillates around the
  initial value. RK4 preserves nothing, it's just accurate, and
  accuracy isn't enough.

*** What Is Confusing or Didn't Work
- It is not easy to calculate the area of phase space cloud plots, so
  it's not easy to carefully verify the area conserving properties of
  symplectic integrators.
- For trajectory matching with oscillatory systems, the loss landscape is:
  - Non-convex: multiple minima
  - Periodic: phase shifts create repeating patterns of good/bad fits
  - Sensitive to initial guesses
  So it's actually quite hard to use standard gradient descent and
  trust the sign of the gradient at any given guess.

*** Open Questions

- I have fetched a copy of a book called "Geometric Numerical
  Integration" that I need to make time to read. It is well reviewed.
- I need to read SICM much more carefully, now that I understand
  Scheme a lot better. Possibly redo parts of it in JAX.

*** Refinements to the Vision
- We don't penalise the drift. We chose an integrator that
  geometrically /cannot/ drift.
** 2025-12-24: Noether's Theorem as an Algorithm

Today, we use Noether's theorem to identify what quantities ought to
be conserved, and verify them numerically. We then use these
quantities to completely change the game on parameter discovery.

*** What I Built/Learnt
**** Reading about Noether's theorem
- https://en.wikipedia.org/wiki/Noether%27s_theorem

  Noether’s theorem says that for a system with conservative forces
  (described by an action/Lagrangian), every continuous, smooth
  symmetry of the action implies a corresponding conserved quantity.
  It underpins much of modern theoretical physics by linking
  symmetries to conservation laws, but it generally doesn’t apply to
  dissipative (non-conservative) systems where a Lagrangian alone
  isn’t enough.

  And because it makes these connections, you can e.g. use the
  knowledge that something is conserved to restrict/suggest functional
  forms of the Lagrangian. (Similarly the other way around, of course.)

  Specifically:

  - Time translation → energy
  - Space translation → momentum
  - Rotation → angular momentum
- https://mitp-content-server.mit.edu/books/content/sectbyfn/books_pres_0/9579/sicm_edition_2.zip/chapter001.html#h1-6c
- https://en.wikipedia.org/wiki/Lagrangian_mechanics#Cyclic_coordinates_and_conserved_momenta

  If a generalised coordinate is not present in the functional form of
  the Lagrangian, it's corresponding conjugate momentum is conserved.
**** Some salient points
***** Noether's Theorem: Key Concepts
****** Cyclic coordinates (simplest case)
If $\frac{\partial L}{\partial q_i} = 0$, coordinate $q_i$ doesn't
appear in $L$. This is a symmetry: translations in $q_i$ leave physics
unchanged. The conjugate momentum $p_i = \frac{\partial L}{\partial
\dot{q}_i}$ is conserved.

****** General symmetries
A continuous symmetry is a direction you can "nudge" the trajectory
without changing the action. The conserved quantity is the projection
of the system's momenta onto the symmetry's infinitesimal generator:
$Q = \sum_i p_i \xi_i$ where $\xi_i = \frac{d}{ds}F_s(q_i)\big|_{s=0}$
is the generator.

****** 3. Why energy is special
- Space translation ($x \to x + \epsilon$): conserves momentum
  (component of motion in that direction)
- Time translation ($t \to t + \epsilon$): conserves energy (the
  Hamiltonian)

Energy is tied to invariance under "when"—whether $L$ has explicit
time dependence. Break that, and energy conservation fails.

***** SICM's approach
- Lagrangian as a procedure on local state $(t, q, \dot{q})$
- Operators that differentiate symbolically
- For Noether: provide transformation family $F_s$, differentiate at
  $s=0$ to get generator $\xi$, compute $p \cdot \xi$

**** Expanding the symbolic mechanics with some numerical verification
***** Detecting cyclic coordinates
This is really straightforward. We just loop over all the coordinates
and take the derivative of L with respect to each of them. We can
easily identify all of the cases where the derivative is 0.

Except it's not as trivial because telling if an expression is 0 is
not decidable.
***** Deriving the Hamiltonian from the Lagrangian
This is a straightforward Legendre transform.
***** Find the quantity conserved by a symmetry
Given an infinitesimal generator, ξ, compute the Noether charge (the
conserved quantity). Then we systematically and numerically verified
this on a real trajectory.
***** Time translation is special
We verified this numerically in Day 4.
**** The clear pattern for specifying a symmetry

1. Write the finite transformation $q \to F_\epsilon(q)$
2. Expand to first order in $\epsilon$
3. Extract the coefficient: $\xi = \frac{d}{d\epsilon}F_\epsilon(q)\big|_{\epsilon=0}$

For translation in $x$: $F_\epsilon(x) = x + \epsilon$, so $\xi = (1, 0)$.

For rotation: $\xi = (-y, x)$.
**** Trying for loss functions that are more robust than trajectory differences
In the previous days, I have struggled with the flakiness of parameter
estimation based on a loss function that was a squared
trajectory-difference.

Today, instead of matching trajectories point-by-point, I try to match
by statistics that don't depend on phase. And it worked gloriously!
*** What Surprised Me
- It's non-trivial (and in fact undecidable) as to whether we can tell
  if a generic expression is 0. This is Richardson's theorem from Day
  3, but I was reminded of it again.
- With Verlet integration in the 2D oscillator example, angular
  momentum error was bounded at O(1e-5) over 10 million steps. The
  oscillatory, non-drifting character showed the symplectic integrator
  respecting the structure.
- Using an energy statistic made the parameter discovery so much
  faster. Physics tells us what is invariant. And we use that
  invariance in our loss function, and the optimisation landscape
  respects the physics instead of fighting it.

  This was the most glorious finding of the day.
*** What Is Confusing or Didn't Work
- I am not completely confident on how to specify the infinitesimal
  generator, ξ. I have a general pattern now, but I need to study it
  further with more complex examples.
*** Open Questions
- It's interesting to me how all of this theory is in
  conservative/non-dissipative systems. I am curious how it extends to
  realistic systems.
*** Refinements to the Vision
- It's clearer I need a =System= object that carries the Lagrangian
  and its derived structure together.

** 2025-12-25: Creating the Abstraction

We have a few key pieces of symbolic mechanics working. Today, we try
to compose them into a coherent API. And then we build out a small
catalogue of common systems. And finally, we get to a place where we
can compose these systems and analyse the composed system.

*** What I Built/Learnt
**** Looking at Diffrax to understand how the API feels
***** Installing Diffrax

#+begin_src bash
pip install diffrax
pip freeze > requirements-tmp.txt
diff requirements-tmp.txt requirements-dev.txt | grep '^<' | sed -e 's/< //g' > requirements.txt
rm requirements-tmp.txt
#+end_src

***** Reading

- https://docs.kidger.site/diffrax/usage/getting-started/
- https://docs.kidger.site/diffrax/api/diffeqsolve/

***** Some opinions and design insights

In general, I do not find its API aesthetically pleasing.

- How does Diffrax separate "what to solve" from "how to solve it"?

  Diffrax has one main entry-point into its API: diffeqsolve(). This
  takes a lot of parameters that include the first parameter terms,
  which is a pytree. It can be as simple as a lambda function, but
  more complex through connecting up terms in a tree through some
  slightly clunky DSL, e.g. MultiTerm(ODETerm(drift),
  ControlTerm(diffusion, brownian_motion)). It also has a parameter
  for the solver to be used, called solver. This is an abstract
  interface to a set of pre-built solvers.

- What's passed at construction time vs call time?

  Call-time: you build the static objects (ODETerm, Dopri5(), SaveAt,
  controllers, etc.) and invoke diffeqsolve; if you jit it, this first
  call also traces your vector field once to build/compile the
  program. Runtime: the compiled solve then repeatedly evaluates your
  term(s) (i.e., calls the vector field and any control increments)
  each step/stage (plus root-finding for implicit methods, step-size
  control, and saving), with no Python per-step execution.

- How are options like tolerances and step sizes handled?

  They are handled dynamically using a set of methods that conform to
  an abstract step size controller. You can tell it to be constant (or
  not), or tell it some initial timestep and some tolerances (relative
  and absolute) and it will use one of a handful of prescribed
  algorithms to dynamically vary the stepsize.

**** Converting the standalone functions we've made so far into a coherent system

The goal of this exercise is to make an API that is powerful and
elegant. It will allow novices to get the right (as in physically
correct, computationally accurate and efficient) answers without
knowing the details. Then,later on, as they get expertise, the have
full control as everything is modular and composable and swappable.

**** A catalogue of common systems

Based on this coherent =LagrangianSystem= class that was built, I then
built out numerous classic pendulums and oscillators. And now, one can
do things like:

#+begin_src python
import jax.numpy as jnp
from contravariant.catalog import harmonic_oscillator_2d

# One line to get a fully-analysed system
sys = harmonic_oscillator_2d()
print(sys)
print()

# Rotation symmetry → angular momentum
x, y = sys.coordinates
L_z = sys.conserved_quantity([-y, x])
print(f"Angular momentum: L_z = {L_z}")
print()

# Compare integrators
params = {'m': 1.0, 'k': 1.0}
state_0 = jnp.array([1.0, 0.0, 0.0, 1.0])

sys.compare_integrators(
    state_0, n_steps=100000, dt=0.01, params=params,
    quantities={'L_z': L_z},
    save_as='2d_oscillator'
)

# Learn parameters
params_true = {'m': 1.0, 'k': 2.0}
traj_observed = sys.integrate(state_0, 1000, 0.01, params_true)

result = sys.learn_parameters(
    traj_observed, state_0, n_steps=1000, dt=0.01,
    params_fixed={'m': 1.0},
    params_init={'k': 0.5},
)

print(f"True k = {params_true['k']}, Learned k = {result['k']:.4f}")
#+end_src

**** A composition system

This was the most beautiful part of the day. By making the individual
pieces just a bit more customisable (e.g. changing the parameters of a
1D oscillator) and introducing an =__add()__= to the
=LagrangianSystem= class, suddenly one can trivially reproduce a 2D
oscillator from 2 different 1D oscillators /added/ together.

#+begin_src python
# Compose 2D isotropic oscillator
sys_x = harmonic_oscillator(coord='x')
sys_y = harmonic_oscillator(coord='y')
sys_2d = sys_x + sys_y
#+end_src

This is glorious!

The composition works because of sympy's symbol caching:
=symbols('m')= called twice returns the /same/ object. So when adding
Lagrangians, shared parameter names automatically unify. This is
mathematically correct—when a physicist writes =L = L_1 + L_2= with
both containing =m=, they mean the same =m=.

*** What Surprised Me

- What was essentially standalone in the last days came together quite
  quickly into one standard class for Lagrangian System.

*** What Is Confusing or Didn't Work

**** Problems illustrated by the Central Force Problem

When I tried a lot of things at once on the Central Force Problem
(Kepler-like), many things didn't work out:

- There's a subtle reason (depending on the coordinates used?) that
  results in terms being non-separable, which means I couldn't use the
  Verlet integrator.
- The non-separability is coordinate-dependent, not a bug. In polar
  coordinates, $T = \frac{1}{2}m(\dot{r}^2 + r^2\dot{\theta}^2)$
  depends on $r$, not just velocities. The same physics in Cartesian
  would be separable. This is a deep point: separability is a property
  of the coordinate representation, not the physics.
- RK4 drift accumulates (which is why structure preservation matters)
- Statistics-based loss doesn't always result in the right direction
  of the gradient for optimisation. This does make me feel like the
  whole business if flaky.
- Underdetermined problems find a solution, not the solution.

*** Open Questions

Should I contribute to diffrax? I noticed when trying to learn from it
that Diffrax has only SemiImplicitEuler for symplectic: no
Störmer-Verlet, no higher-order symplectic methods. Symplectic
Runge-Kutta (implicit midpoint, Gauss-Legendre), splitting methods
(Yoshida, Forest-Ruth), and variational integrators are all missing.

*** Refinements to the Vision

There was something very deep about the modularity/composability that
was shown late today that I would like to never forget and always
build on.

** 2025-12-26: Refining the Abstraction

Today, we just exercise the system in different ways and sees what
works and what needs refinement.

*** What I Built/Learnt

**** Surveying the systems we have

When surveying all the systems we have, the following insights pop out:

- Separable systems (simple harmonic, coupled, anharmonic oscillators
  and simple pendulum) use Verlet integration which results in O(1e-5)
  to O(1e-4) bounded, oscillatory error.
- The spherical pendulum is non-separable but integrable. RK4 does
  fine (6e-6). The cyclic coordinate detection worked beautifully: φ is
  cyclic, p_φ conserved to 2e-6.
- The double pendulum is non-separable /and/ chaotic. 5.3e-3 is 1000x
  worse. This isn't RK4's fault, it's chaos. After the Lyapunov time,
  trajectories diverge exponentially regardless of integrator.

**** A higher order simplectic integrator (Yoshida Algorithms)

Because we've been comparing a much higher RK4 to the Verlet, all
results (while shaped ok) look skewed in numeric terms. So today I
fixed that and implemented one algorithm from Yoshida. This is fourth
order.

- https://en.wikipedia.org/wiki/Leapfrog_integration#Yoshida_algorithms
- https://www.sciencedirect.com/science/article/abs/pii/0375960190900923 (can't access)

**** A summary of results

| System              | Separable | Integrator | Energy Err | Learn | Extra Q |
|---------------------+-----------+------------+------------+-------+---------|
| SHO                 | ✓         | Both       | O(1e-6)    | ✓     | —       |
| Coupled Oscillators | ✓         | Both       | O(1e-6)    | ✓     | —       |
| Anharmonic          | ✓         | Both       | O(1e-6)    | ✓     | —       |
| Simple Pendulum     | ✓         | Both       | O(1e-6)    | ✓     | —       |
| Double Pendulum     | ✗         | RK4        | O(1e-3)    | skip  | —       |
| Spherical Pendulum  | ✗         | RK4        | O(1e-6)    | ✓     | p_φ     |

**** Adding particles to the catalogue

It turns out we can also do particles floating in free space and under
central forces easily with this system. So now we have a free particle
in 2d, moving under a central force and Lepler's system as examples.

**** A deeper look at just the double pendulum

This is such a simple extension, at least in the Lagrangian space, but
it leads to a lot of complexity and beauty:

- Non-separable: kinetic energy couples velocities
  (l₁l₂m₂θ̇₁θ̇₂cos(θ₁-θ₂)), so only RK4 works
- RK4 with float64 achieves O(1e-8) energy error over 100s:
  sufficient for physics
- Chaos: 1e-9 rad perturbation grows to O(1) in ~23s (exponential,
  then saturates)
- Lyapunov exponent λ ≈ 0.7 s⁻¹ means mass × 2.7 error growth each 1.4
  seconds
- Phase space: mass 1 oscillates (θ₁ bounded), mass 2 rotates (θ₂
  unbounded)
- Energy regimes: low → quasi-periodic, medium → chaotic, high →
  rotational
- Loss landscape is fractal: gradient descent cannot find minimum
- Poincaré section reduces 4D → 2D but needs long integration for
  structure
- Parameter learning fundamentally fails for chaos: ∂traj/∂param is
  uninformative

**** More interesting forms of composition

As of yesterday, we composed systems with adding =LagrangianSystem=
objects. And this worked for systems that were fundamentally
decoupled.

Today, this was extended in a way that was trivial in code but quite
deep in impact. With the generic ability to =add= and =sub= both these
system objects and just general sympy exprs, we can do what physicists
do every day, which is to both add Lagrangians and then add (kinetic)
or subtract (potential) coupling terms.

This allowed us to model more systems.

*** What Surprised Me

**** Double pendulum is a fairly simple extension that ups complexity by a lot

The double pendulum exposes a fundamental limit. For chaotic systems:

- Trajectory accuracy degrades exponentially
- No integrator fixes this
- But statistical properties (energy distribution, Lyapunov exponents) can still be meaningful

**** Because of RK4's accuracy, in many practical cases it has better conservation properties

Surprise: RK4 has lower max error than Verlet at 10k steps. This is
expected short-term—RK4 is O(h⁴) accurate, Verlet is O(h²). The
symplectic advantage appears at long times when RK4 drifts
monotonically and Verlet stays bounded. We saw this at 100k steps
before.

So today, we introduced a higher-order symplectic method, from
Yoshida.

****  RK4 vs Yoshida: What Symplectic Actually Means

***** The Expectation

Symplectic integrators like Störmer-Verlet and Yoshida preserve the
symplectic structure of Hamiltonian phase space. The textbook claim is
that this leads to *bounded* energy error over arbitrarily long times,
while non-symplectic methods like RK4 accumulate drift. We implemented
Yoshida (4th-order symplectic) to compare fairly with RK4 (4th-order,
not symplectic) at the same accuracy order.

***** The Reality

The experiments revealed a more nuanced picture:

****** Simple Harmonic Oscillator (1 billion steps):

#+begin_src
rk4:     1.90e-05
yoshida: 2.40e-05
#+end_src

Both bounded, RK4 slightly better. The SHO is linear, so RK4's errors
don't compound—they oscillate rather than drift.

****** Anharmonic Oscillator (quartic potential):

#+begin_src
                   100k        1M          10M
rk4:             5.25e-06    2.69e-05    2.92e-05
yoshida:         2.10e-05    1.00e-04    3.29e-04
#+end_src

RK4 stays bounded; Yoshida's error appears to grow. But plotting the
error over time reveals both are oscillating around fixed
offsets—neither is drifting monotonically. RK4 simply has a smaller
systematic offset for this system.

****** Simple Pendulum (trigonometric nonlinearity)

#+begin_src
                   100k        1M          10M
rk4:             5.15e-05    2.40e-04    8.29e-04
yoshida:         2.72e-04    3.48e-04    3.48e-04
#+end_src

Here the crossover happens! At 100k steps, RK4 is 5× better. By 10M
steps, Yoshida is 2.4× better. RK4 drifts (5e-5 → 8e-4 = 16× growth);
Yoshida stays bounded (2.7e-4 → 3.5e-4 = 1.3× growth).

***** The Explanation

Symplectic integrators exactly preserve a modified Hamiltonian H̃ =
H + O(h^n), not the true Hamiltonian H. The offset from true energy is
bounded but nonzero. RK4 is optimized for accuracy (minimizing the
leading error coefficient). Yoshida is optimized for structure
preservation (exact conservation of H̃), which may have a larger
coefficient.

Whether the symplectic advantage manifests depends on:

1. System dynamics: For some systems (pendulum), RK4's small per-step
   errors accumulate into drift. For others (anharmonic oscillator),
   RK4 accidentally stays bounded.
2. Timestep relative to dynamics: The anharmonic oscillator with
   dt=0.01 showed Yoshida struggling; at dt=0.001, both methods
   performed comparably. Symplectic integrators still have stability
   limits.
3. Integration length: The symplectic advantage requires enough time
   for RK4's drift to exceed Yoshida's larger constant offset. For
   short simulations, RK4's smaller constant wins.

***** The Takeaway

Symplectic integration guarantees bounded energy error within the
stability region, but "bounded" doesn't mean "smallest." RK4 can
outperform symplectic methods when:
- The system happens to be well-behaved for RK4
- The timestep is small enough that drift hasn't accumulated
- The integration isn't long enough for drift to dominate

The real value of symplectic integration appears in:
- Celestial mechanics (millions of orbits)
- Molecular dynamics (billions of steps)
- Any setting where phase space geometry matters (KAM theory,
  resonances)
- Systems where you need *guarantees*, not just good average behaviour

The framework offers both. The default (`method='auto'`) selects
Yoshida for separable systems—a safe choice that's never
catastrophically wrong. Experts can select RK4 when they know their
system and prefer the smaller error constant for shorter integrations.

*** What Is Confusing or Didn't Work

**** It is not clear to me when errors compound

It turns out RK4 is astounding for the simple harmonic oscillator.
Even after 1B timesteps, it can come out ahead of an equivalent order
symplectic integrator.

#+begin_src
In [1]: import jax.numpy as jnp

In [2]: from contravariant.catalog import harmonic_oscillator

In [3]: # Quick test: 1B steps
   ...: sys = harmonic_oscillator()
   ...: params = {'m': 1.0, 'k': 1.0}
   ...: state_0 = jnp.array([1.0, 0.0])
   ...:
   ...: for method in ['rk4', 'yoshida']:
   ...:     traj = sys.integrate(state_0, 1_000_000_000, 0.01, params, method=method)
   ...:     cons = sys.check_conservation(traj, params)
   ...:     print(f"{method}: {cons['energy'][0]:.2e}")
   ...:
rk4: 1.90e-05
yoshida: 2.40e-05
#+end_src

**** Parameter learning failed on the Kepler problem

The sign is wrong and it's not converging. The energy_statistic loss
(matching mean q² and mean v²) doesn't uniquely identify k for the
Kepler problem. The loss landscape has local minima or the statistics
are degenerate for different k values. This is a known limitation of
statistical loss functions for certain systems. The Kepler problem
would need a different loss function (maybe period matching, or
trajectory loss with careful phase alignment).

*** Open Questions

*** Refinements to the Vision

**** This is less visionary, more just a list of things to do

- Loss functions beyond energy_statistic (period matching, orbital
  elements, Poincaré statistics)
- Holonomic constraints (pendulum on moving pivot, bead on wire)
- Implicit integrators for stiff potentials
- Automatic float64 promotion when chaos detected

** 2025-12-27: The Fermi-Pasta-Ulam-Tsingou Problem

The Fermi-Pasta-Ulam-Tsingou problem (1955) is one of the founding
puzzles of nonlinear dynamics. The authors setup a set of points in 1D
space joined by mostly linear springs (with a small, like 10% addition
of nonlinear terms of higher order) and set it off with different
initial conditions.

Instead of seeing "thermalization," i.e. the energy just got randomly
distributed across different modes of vibration, for some values of
parameters, they got recurrence instead.

We reproduce large parts of these experiments today.


*** What I Built/Learnt
**** Learning about the problem

- https://www.osti.gov/biblio/4376203
- http://www.scholarpedia.org/article/Fermi-Pasta-Ulam_nonlinear_lattice_oscillations

The FPUT model is a chain of $N$ masses with nearest-neighbour
springs. The linear part is just the discrete wave equation (a lattice
Laplacian): $m\ddot{x}_j = k(x_{j+1} + x_{j-1} - 2x_j)$. In
normal-mode coordinates this linear system decouples, so each mode
oscillates independently and energy stays exactly in whatever mode you
initially excite.

FPUT then add a weak nonlinearity to the spring force. In the
$\alpha$ FPUT model the force has a quadratic correction—the potential
gains a cubic term $\propto (\Delta x)^3$, so the force acquires an
additional term quadratic in the displacement:

$m\ddot{x}_j = k(x_{j+1} + x_{j-1} - 2x_j) + \alpha\bigl[(x_{j+1} - x_j)^2 - (x_j - x_{j-1})^2\bigr]$

In the $\beta$ FPUT model the correction is cubic in the spring
extension (quartic in the potential), typically written in terms of
$\Delta x = x_{j+1} - x_j$ as a force $\propto \Delta x + \beta(\Delta
x)^3$.

What you see depends mainly on nonlinearity strength (small $\alpha$
or $\beta$) and the initial condition (energy placed in one
low-frequency mode). For weak nonlinearity and low-mode initial data,
energy leaks into a few other modes but then returns: an
almost-periodic FPUT recurrence rather than quick equipartition. As
energy or $\alpha$, $\beta$ increase, more modes couple strongly, the
motion looks more irregular, and the system tends toward equipartition
(thermalization), often on a timescale that becomes very long as
$\alpha, \beta \to 0$ because effective resonant mixing is weak.

**** Experiments with a simple implementation of the problem

After adding the system to our catalogue, I ran some experiments.

***** Verify conservation

At N = 32, the energy is basically conserved exactly, whether we use
RK4 or Yoshida (it's basically within O(1.e-5)).

Whereas at N=16 (or 8), something really interesting happens. the
motion has gone to that chaotic place and the RK4 falls off a cliff,
while Yoshida keeps going strong.

- TODO Quasi-momentum from discrete translation (if periodic BC)

***** The FPUT recurrence

By trying out for 1,000,000 time steps, with different beta and
amplitude result in different regimes:

- Low amplitude/beta: Energy basically stays in first mode, where it
  started.
- Medium amplitude/beta: Some energy goes into mods 3 and 5 and
  returns to 1 in a beautiful, cyclic fashion.
- High amplitude/beta: Completely chaotic, energy just goes into all
  different modes and never recurs.

***** Scaling test

The following is how the experiments scale with the number of points
in the system. I feel like both the numerics and the symbolic stuff
scale with =N=.

|   N | Time     |
|   8 | 3.729 s  |
|  16 | 17.706 s |
|  32 | 43.820 s |
|  64 | 57.741 s |
| 128 | 100.46 s |

FPUT Scaling (1M steps, dt=0.1):

- Symbolic derivation: O(N), ~0.2s per DOF
- Integration: O(N) per step, but ~20× penalty crossing N≈16
  (likely cache threshold)
- For N=32: ~40K steps/sec → 1 second of simulation per second of wall time
  (at dt=0.1, so T=100,000 per 1000s wall time)

**** Momentum conservation

When the boundaries are fixed, the momentum of this system is not
conserved. The walls break translational symmetry. But when the
boundary is periodic, this is not the case.

This can be verified both numerically:

#+begin_src
Periodic BC:
  P(0) = 8.000000
  P(T) = 8.000112
  max |ΔP| = 1.17e-04

Fixed BC:
  P(0) = 7.995241
  P(T) = 1.053632
  max |ΔP| = 1.53e+01
#+end_src

and symbolically:

#+begin_src
δL (periodic): 0

δL (fixed): -4*beta*q0**3 - 4*beta*q15**3 - k*q0 - k*q15
#+end_src

**** Animating the results

I got to animate the overall movement of the nodes (in the y-axis) and
plot it against the energy in different modes. It shows the entire
behaviour of energy beginning in mode 1, flowing to the others and
back, cyclically.

**** Completely reorganised the experiments

#+begin_src
# 1. Install your package in "editable" mode
# This installs dependencies from requirements.txt AND makes the 'contravariant' folder importable
pip install -e .

# 2. Install development tools separately
pip install -r requirements-dev.txt
#+end_src

*** What Surprised Me

**** Why Symplectic Integrators Matter

The conventional wisdom is that higher-order integrators are "more
accurate." RK4 and Yoshida are both O(h⁴) — they make the same size
error per step. So why choose one over the other?

The answer only becomes clear over long times, and especially in
chaotic systems. RK4's errors are generic: each step slightly distorts
phase space volume, and these distortions accumulate. In chaotic
dynamics, small errors grow exponentially, and the systematic drift
compounds into catastrophic energy loss (or gain). Yoshida's errors
are constrained: as a symplectic integrator, it exactly preserves the
symplectic 2-form, which means it cannot systematically drift in
energy. The errors oscillate but remain bounded, even as the
underlying dynamics becomes chaotic.

The FPUT experiment reveals this starkly. At N=32 in the
near-integrable regime, both integrators perform comparably. i.e The
dynamics is "nice" and errors stay small. At N=8, the same total
energy is concentrated in fewer modes, pushing the system into chaos.
Here RK4 loses 75% of its energy over T=100,000 while Yoshida
oscillates within 10⁻⁵. Same equations, same timestep, same formal
accuracy — but fundamentally different long-term behavior. Symplectic
integrators don't win by being more accurate per step; they win by not
accumulating the kind of errors that destroy conservation laws. This
is why they're essential for molecular dynamics, celestial mechanics,
and any simulation where you need to trust the physics over millions
of timesteps.

*** What Is Confusing or Didn't Work

- Initial parameter tuning to find the recurrence regime (too weak →
  linear, too strong → chaos)
- JAX async dispatch hiding true timings until =.block_until_ready()=
- The symbolic conserved_quantity not checking if the symmetry holds

*** Open Questions
*** Refinements to the Vision
** 2025-12-28: Polishing the Abstraction

Today was mostly auto-generated with a lots of experimentation and
subtle improvements.

*** What I Built/Learnt

- Time-dependent Lagrangians are automatically recognised as not
  energy conserving.
- Ensure that sympectic methods cannot be used on non-separable
  systems.
- Be a bit more graceful about =sympy= failures in trying to solve the
  Euler-Lagrange equations.
- Ensure that all parameters that are needed are passed into the
  integrators.
- Expand on docstrings for education.
- Add a whole test suite for the catalogue its behaviour as well as
  edge cases.

*** What Surprised Me
*** What Is Confusing or Didn't Work
*** Open Questions
*** Refinements to the Vision
